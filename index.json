[{"authors":null,"categories":null,"content":"I am a computational linguist with a background in NLP and in psycholinguistics. I am interested in\n data-centered AI: how we can improve data quality to train better machine learning models on language tasks human-centered AI, that is how we can take humans (users, designers, crowdworkers and annotators) into account in all things AI to make the best of their intuitions and to interact with them in a fair way  ","date":1636588800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1636588800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a computational linguist with a background in NLP and in psycholinguistics. I am interested in\n data-centered AI: how we can improve data quality to train better machine learning models on language tasks human-centered AI, that is how we can take humans (users, designers, crowdworkers and annotators) into account in all things AI to make the best of their intuitions and to interact with them in a fair way  ","tags":null,"title":"Alessandra Zarcone","type":"authors"},{"authors":[],"categories":null,"content":"","date":1656428400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656428400,"objectID":"97cd506a4bb128c0e9c490723b58162e","permalink":"http://azarcone.github.io/talk/data-and-user-centric-nlp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/data-and-user-centric-nlp/","section":"event","summary":"Invited talk for the Internationalen Hochschule AI Impulse Serie Talk at the","tags":[],"title":"Data- and user-centric NLP","type":"event"},{"authors":[],"categories":null,"content":"","date":1655298000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655298000,"objectID":"527e9d18177b1f1691d530e7e304cb22","permalink":"http://azarcone.github.io/talk/conversational-ai-between-hype-and-hope-a-case-for-data-and-human-centric-approaches/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/conversational-ai-between-hype-and-hope-a-case-for-data-and-human-centric-approaches/","section":"event","summary":"Invited talk for one of the weekly seminar at CLASP, University of Gothenburg.","tags":[],"title":"Conversational AI between hype and hope – A case for data- and human-centric approaches","type":"event"},{"authors":[],"categories":null,"content":"","date":1653505200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653505200,"objectID":"0a5e89b0ef60c5528c0d4d3d325999d0","permalink":"http://azarcone.github.io/talk/wie-alexa-sprechen-lernt-sprachassistenz-zwischen-datenhunger-und-datenschutz/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/wie-alexa-sprechen-lernt-sprachassistenz-zwischen-datenhunger-und-datenschutz/","section":"event","summary":"Die Vorlesung is teil einer großangelegten, öffentlichen Ringvorlesung der Hochschule Augsburg, die in zehn Veranstaltungen im Sommersemester 2022 die Auswirkungen Künstlicher Intelligenz auf unser gesellschaftliches Zusammenleben untersucht.","tags":[],"title":"Wie Alexa sprechen lernt – Sprachassistenz zwischen Datenhunger und Datenschutz","type":"event"},{"authors":["Lianna Hrycyk","Alessandra Zarcone","Luzian Hahn"],"categories":null,"content":"","date":1636588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636588800,"objectID":"00ea854d1453538d3a661dde8de9c6a5","permalink":"http://azarcone.github.io/publication/2021-nlp4convai/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-nlp4convai/","section":"publication","summary":"Incremental intent classification requires the assignment of intent labels to partial utterances. However, partial utterances do not necessarily contain enough information to be mapped to the intent class of their complete utterance (correctly and with a certain degree of confidence). Using the final interpretation as the ground truth to measure a classifier’s accuracy during intent classification of partial utterances is thus problematic. We release inCLINC, a dataset of partial and full utterances with human annotations of plausible intent labels for different portions of each utterance, as an upper (human) baseline for incremental intent classification. We analyse the incremental annotations and propose entropy reduction as a measure of human annotators’ convergence on an interpretation (i.e. intent label). We argue that, when the annotators do not converge to one or a few possible interpretations and yet the classifier already identifies the final intent class early on, it is a sign of overfitting that can be ascribed to artefacts in the dataset.","tags":["crowdsourcing","data-centric","human upper bound","incremental intent classification","incremental processing","Information Density","Information Theory","NLU"],"title":"Not So Fast, Classifier – Accuracy and Entropy Reduction in Incremental Intent Classification","type":"publication"},{"authors":["Alessandra Zarcone","Vera Demberg"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"7aade27170b204d4c6e4fd862ec84caa","permalink":"http://azarcone.github.io/publication/2021-cogsci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-cogsci/","section":"publication","summary":"The Uniform Information Density hypothesis (UID) predicts that lexical choice between long and short word forms depends on the predictability of the referent in context, and recent studies have shown such an effect of predictability on lexical choice during online production. We here set out to test whether the UID predictions hold up in a related setting, but different language (German) and different phenomenon, namely the choice between compounds (e.g. Badewanne / bathtub) or their base forms (Wanne / tub). Our study is consistent with the UID: we find that participants choose the shorter base form more often in predictive contexts, showing an active tendency to be information-theoretically efficient.","tags":["crowdsourcing","German compounds","expectation-based processing","incremental processing","Information Density","Information Theory","predictability","Rational Speech Act framework","referring expressions","script knowledge","Uniform Information Density hypothesis"],"title":"A bathtub by any other name: the reduction of German compounds in predictive contexts","type":"publication"},{"authors":["Alessandra Zarcone","Vera Demberg"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"0c497bd37c85ef6cf6491197e555640e","permalink":"http://azarcone.github.io/publication/2021-discproc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-discproc/","section":"publication","summary":"There is now a well-established literature showing that people anticipate upcoming concepts and words during language processing. Commonsense knowledge about typical event sequences and verbal selectional preferences can contribute to anticipating what will be mentioned next. We here investigate how temporal discourse connectives (before, after), which signal event ordering along a temporal dimension, modulate predictions for upcoming discourse referents. Our study analyses anticipatory gaze in the visual world and supports the idea that script knowledge, temporal connectives (before eating → menu, appetizer), and the verb’s selectional preferences (order → appetizer) jointly contribute to shaping rapid prediction of event participants.","tags":["discourse","expectation-based processing","incremental processing","script knowledge","world knowledge","visual world paradigm"],"title":"Interaction of Script Knowledge and Temporal Discourse Cues in a Visual World Study","type":"publication"},{"authors":["Yannick Frommherz","Alessandra Zarcone"],"categories":null,"content":"","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624233600,"objectID":"98ad4b6581ce18c8866a24e529317e25","permalink":"http://azarcone.github.io/publication/2021-frontcomputsci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-frontcomputsci/","section":"publication","summary":"Despite their increasing success, user interactions with smart speech assistants (SAs) are still very limited compared to human-human dialogue. One way to make SA interactions more natural is to train the underlying natural language processing modules on data which reflects how humans would talk to a SA if it was capable of understanding and producing natural dialogue given a specific task. Such data can be collected applying a Wizard-of-Oz approach (WOz), where user and system side are played by humans. WOz allows researchers to simulate human-machine interaction while benefitting from the fact that all participants are human and thus dialogue-competent. More recent approaches have leveraged simple templates specifying a dialogue scenario for crowdsourcing large-scale datasets. Template-based collection efforts, however, come at the cost of data diversity and naturalness. We present a method to crowdsource dialogue data for the SA domain in the WOz framework, which aims at limiting researcher-induced bias in the data while still allowing for a low-resource, scalable data collection. Our method can also be applied to languages other than English (in our case German), for which fewer crowd-workers may be available. We collected data asynchronously, relying only on existing functionalities of Amazon Mechanical Turk, by formulating the task as a dialogue continuation task. Coherence in dialogues is ensured, as crowd-workers always read the dialogue history, and as a unifying scenario is provided for each dialogue. In order to limit bias in the data, rather than using template-based scenarios, we handcrafted situated scenarios which aimed at not pre-script-ing the task into every single detail and not priming the participants’ lexical choices. Our scenarios cued people’s knowledge of common situations and entities relevant for our task, without directly mentioning them, but relying on vague language and circumlocutions. We compare our data (which we publish as the CROWDSS corpus; n = 113 dialogues) with data from MultiWOZ, showing that our scenario approach led to considerably less scripting and priming and thus more ecologically-valid dialogue data. This suggests that small investments in the collection setup can go a long way in improving data quality, even in a low-resource setup.","tags":["crowdsourcing","data-centric","dialogue","ecological data","Human-Machine Interaction","Wizard-of-Oz"],"title":"Crowdsourcing ecologically-valid dialogue data for German","type":"publication"},{"authors":["Touhidul Alam","Alessandra Zarcone","Sebastian Padó"],"categories":null,"content":"","date":1623801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623801600,"objectID":"6b5cbedde421154443fcb6456970bf79","permalink":"http://azarcone.github.io/publication/2021-iwcs/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-iwcs/","section":"publication","summary":"Reliable tagging of Temporal Expressions (TEs, e.g., Book a table at L’Osteria for Sunday evening) is a central requirement for Voice Assistants (VAs). However, there is a dearth of resources and systems for the VA domain, since publicly-available temporal taggers are trained only on substantially different domains, such as news and clinical text. Since the cost of annotating large datasets is prohibitive, we investigate the trade-off between in-domain data and performance in DA-Time, a hybrid temporal tagger for the English VA domain which combines a neural architecture for robust TE recognition, with a parser-based TE normalizer. We find that transfer learning goes a long way even with as little as 25 in-domain sentences: DA-Time performs at the state of the art on the news domain, and substantially outperforms it on the VA domain.","tags":["domain adaptation","NLU","temporal expressions","temporal tagging","transfer learning","voice assistants"],"title":"New Domain, Major Effort? How Much Data is Necessary to Adapt a Temporal Tagger to the Voice Assistant Domain","type":"publication"},{"authors":[],"categories":null,"content":"","date":1619010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619010000,"objectID":"c37fb9cd8c648c0f21f91bbe49b28bdc","permalink":"http://azarcone.github.io/talk/hey-google-wie-sorgt-man-fur-gendergerechtigtkeit-in-der-maschinellen-sprachverarbeitung/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/hey-google-wie-sorgt-man-fur-gendergerechtigtkeit-in-der-maschinellen-sprachverarbeitung/","section":"event","summary":"Wir werden in diesem Vortrag gemeinsam aufdecken, welche Verzerrungen in datenbasierten Modellen der Sprache auftauchen können und wie man dagegen vorgehen kann.","tags":[],"title":"Hey Google, wie sorgt man für Gendergerechtigtkeit in der Maschinellen Sprachverarbeitung?","type":"event"},{"authors":["Alessandra Zarcone","吳恩達"],"categories":["Demo","教程"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"http://azarcone.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Alessandra Zarcone","Touhidul Alam","Zahra Kolagar"],"categories":null,"content":"","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"79ca6ebd7012e0bf844fe76c195a804c","permalink":"http://azarcone.github.io/publication/2020-lrec/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2020-lrec/","section":"publication","summary":"The recognition and automatic annotation of temporal expressions (e.g. “Add an event for tomorrow evening at eight to my calendar”) is a key module for AI voice assistants, in order to allow them to interact with apps (for example, a calendar app). However, in the NLP literature, research on temporal expressions has focused mostly on data from the news, from the clinical domain, and from social media. The voice assistant domain is very different than the typical domains that have been the focus of work on temporal expression identification, thus requiring a dedicated data collection. We present a crowdsourcing method for eliciting natural-language commands containing temporal expressions for an AI voice assistant, by using pictures and scenario descriptions. We annotated the elicited commands (480) as well as the commands in the Snips dataset following the TimeML/TIMEX3 annotation guidelines, reaching a total of 1188 annotated commands. The commands can be later used to train the NLU components of an AI voice assistant.","tags":["NLU","temporal expressions","temporal tagging","voice assistants"],"title":"PÂTÉ: A Corpus of Temporal Expressions for the In-car Voice Assistant Domain","type":"publication"},{"authors":["Alessandra Zarcone","Ken McRae","Alessandro Lenci","Sebastian Padó"],"categories":null,"content":"","date":1511481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511481600,"objectID":"611d3bd65ef34a83ec9ef3121f90a5cb","permalink":"http://azarcone.github.io/publication/2017-front-psychol/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2017-front-psychol/","section":"publication","summary":"Complement coercion (begin a book → reading) involves a type clash between an event-selecting verb and an entity-denoting object, triggering a covert event (reading). Two main factors involved in complement coercion have been investigated: the semantic type of the object (event vs. entity), and the typicality of the covert event (the author began a book → writing). In previous research, reading times have been measured at the object. However, the influence of the typicality of the subject–object combination on processing an aspectual verb such as begin has not been studied. Using a self-paced reading study, we manipulated semantic type and subject–object typicality, exploiting German word order to measure reading times at the aspectual verb. These variables interacted at the target verb. We conclude that both type and typicality probabilistically guide expectations about upcoming input. These results are compatible with an expectation-based view of complement coercion and language comprehension more generally in which there is rapid interaction between what is typically viewed as linguistic knowledge, and what is typically viewed as domain general knowledge about how the world works.","tags":["complement coercion","expectation-based processing","logical metonymy","lexical meaning in context","self-paced reading","type clash","typicality","world knowledge"],"title":"Complement Coercion: The Joint Effects of Type and Typicality","type":"publication"},{"authors":["Lilian D. A. Wanzare","Alessandra Zarcone","Stefan Thater","Manfred Pinkal"],"categories":null,"content":"","date":1491177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491177600,"objectID":"a789df0360a28909496b1ac1685838a0","permalink":"http://azarcone.github.io/publication/2017-lsdsem/","publishdate":"2017-04-03T00:00:00Z","relpermalink":"/publication/2017-lsdsem/","section":"publication","summary":"We present a semi-supervised clustering approach to induce script structure from crowdsourced descriptions of event sequences by grouping event descriptions into paraphrase sets (representing event types) and inducing their temporal order. Our approach exploits semantic and positional similarity and allows for flexible event order, thus overcoming the rigidity of previous approaches. We incorporate crowdsourced alignments as prior knowledge and show that exploiting a small number of alignments results in a substantial improvement in cluster quality over state-of-the-art models and provides an appropriate basis for the induction of temporal order. We also show a coverage study to demonstrate the scalability of our approach.","tags":["clustering","crowdsourcing","event order","script knowledge"],"title":"Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering","type":"publication"},{"authors":["Alessandra Zarcone","Marten van Schijndel","Jorrig Vogels","Vera Demberg"],"categories":null,"content":"","date":1465171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465171200,"objectID":"7085f1fdc43e7111d35a84e7d8915004","permalink":"http://azarcone.github.io/publication/2016-front-psychol/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2016-front-psychol/","section":"publication","summary":"The notion of salience has been singled out as the explanatory factor for a diverse range of linguistic phenomena. In particular, perceptual salience (e.g., visual salience of objects in the world, acoustic prominence of linguistic sounds) and semantic-pragmatic salience (e.g., prominence of recently mentioned or topical referents) have been shown to influence language comprehension and production. A different line of research has sought to account for behavioral correlates of cognitive load during comprehension as well as for certain patterns in language usage using information-theoretic notions, such as surprisal. Surprisal and salience both affect language processing at different levels, but the relationship between the two has not been adequately elucidated, and the question of whether salience can be reduced to surprisal / predictability is still open. Our review identifies two main challenges in addressing this question: terminological inconsistency and lack of integration between high and low levels of representations in salience-based accounts and surprisal-based accounts. We capitalize upon work in visual cognition in order to orient ourselves in surveying the different facets of the notion of salience in linguistics and their relation with models of surprisal. We find that work on salience highlights aspects of linguistic communication that models of surprisal tend to overlook, namely the role of attention and relevance to current goals, and we argue that the Predictive Coding framework provides a unified view which can account for the role played by attention and predictability at different levels of processing and which can clarify the interplay between low and high levels of processes and between predictability-driven expectation and attention-driven focus.","tags":["attention","expectation-based processing","Information Theory","predictive coding","perceptual salience","predictability","salience","surprisal"],"title":"Salience and Attention in Surprisal-Based Accounts of Language Processing","type":"publication"},{"authors":["Lilian D. A. Wanzare","Alessandra Zarcone","Stefan Thater","Manfred Pinkal"],"categories":null,"content":"","date":1463961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463961600,"objectID":"ff2bc4bd6184b2a13dd384f53a816ad9","permalink":"http://azarcone.github.io/publication/2016-lrec/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2016-lrec/","section":"publication","summary":"Scripts are standardized event sequences describing typical everyday activities, which play an important role in the computational modeling of cognitive abilities (in particular for natural language processing). We present a large-scale crowdsourced collection of explicit linguistic descriptions of script-specific event sequences (40 scenarios with 100 sequences each). The corpus is enriched with crowdsourced alignment annotation on a subset of the event descriptions, to be used in future work as seed data for automatic alignment of event descriptions (for example via clustering). The event descriptions to be aligned were chosen among those expected to have the strongest corrective effect on the clustering algorithm. The alignment annotation was evaluated against a gold standard of expert annotators. The resulting database of partially-aligned script-event descriptions provides a sound empirical basis for inducing high-quality script knowledge, as well as for any task involving alignment and paraphrase detection of events","tags":["crowdsourcing","event order","script knowledge"],"title":"DeScript: A Crowdsourced Corpus for the Acquisition of High-Quality Script Knowledge","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"http://azarcone.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Alessandra Zarcone","Sebastian Padó","Alessandro Lenci"],"categories":null,"content":"","date":1427673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1427673600,"objectID":"1f806f1b8184bc425e784ac5e0744cb4","permalink":"http://azarcone.github.io/publication/2015-networds/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2015-networds/","section":"publication","summary":"We aim to model the results from a self-paced reading experiment, which tested the effect of semantic type clash and typicality on the processing of German complement coercion. We present two distributional semantic models to test if they can model the effect of both type and typicality in the psycholinguistic study. We show that one of the models, without explicitly representing type information, can account both for the effect of type and typicality in complement coercion.","tags":["complement coercion","distributional semantics","expectation-based processing","logical metonymy","lexical meaning in context","type clash","typicality","world knowledge"],"title":"Same same but different: Type and typicality in a distributional model of complement coercion","type":"publication"},{"authors":["Olga Batiukova","Pier Marco Bertinetto","Alessandro Lenci","Alessandra Zarcone"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"7f062f1669068944bec9e53bbda60730","permalink":"http://azarcone.github.io/publication/2015-cahierschronos/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2015-cahierschronos/","section":"publication","summary":"This paper reports four semantic priming experiments in Italian and Spanish, whose goal was to verify the psychological reality of two aspectual features, resultativity and durativity. In the durativity task, the participants were asked whether the verb referred to a durable situation, in the resultativity task if it denoted a situation with a clear outcome. The results prove that both features are involved in online processing of the verb meaning: achievements ([+resultative, -durative]) and activities ([-resultative, +durative]) were processed faster in certain priming contexts. The priming patterns in the Romance languages present some striking similarities (only achievements were primed in the resultativity task) alongside some intriguing differences, and interestingly contrast with the behaviour of another language tested, Russian, whose aspectual system differs in significant ways.","tags":["aspectual features","lexical aspect","lexical meaning in context","semantic priming"],"title":"Identifying actional features through semantic priming: Cross-Romance comparison","type":"publication"},{"authors":["Alessandra Zarcone"],"categories":null,"content":"","date":1409529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409529600,"objectID":"e57de740aa79df6dc59d69a6d796207e","permalink":"http://azarcone.github.io/publication/2014-phdthesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2014-phdthesis/","section":"publication","summary":"During language understanding, people do not only rely on what they read or hear, but they also exploit implicit information. For example, when they process the expression 'begin the book', they understand it involves an event which is not explicitly mentioned (e.g. 'begin reading the book'). This thesis looks at these constructions, known as logical metonymies, which combine an event-selecting verb and entity-denoting object and involve covert events. Logical metonymies are an interesting challenge for theories of lexical semantics: they need to be reconciled with compositionality, they require the integration of context (writers typically write books, students typically read them), and they lie at the interface between lexicon and world knowledge (is the information that books are read stored in our mental lexicon or in our world knowledge?).\nI critically analyze previous hypotheses on logical metonymy with regard to the answer they provide to two core problems: the source problem (what events are retrieved? what type of event knowledge is assumed?) and the trigger problem (why do some constructions trigger a metonymic interpretation and others do not?). Lexicalist approaches claim that the metonymy arises from a type clash between the event-selecting verb and an entity-denoting object, and posit complex lexical items, encoding event information about artifacts (e.g. book → read), to explain the recovery of covert events. Pragmatic-based approaches argue against the idea that lexical items have an internal structure, suggesting that covert events arise from the underspecification of a logical metonymy and are inferred via non-lexical knowledge. I look with particular attention at the role of event knowledge, which lexicalist approaches place in our mental lexicon, while pragmatic-based approaches place it in our world knowledge.\nI propose a third hypothesis, based on thematic fit and generalized event knowledge of typical events and their participants, which have been shown to guide efficient incremental processing: I argue that contextual elements cue generalized event knowledge, which plays a key role in determining the covert event for a logical metonymy. I explore this hypothesis from an interdisciplinary perspective, employing both psycholinguistic experiments and computational models, in order to seek converging evidence and confront it with the theoretical investigation. The results from the psycholinguistic experiments and from the computational (distributional) models support the hypothesis that covert event retrieval is guided by generalized event knowledge. I also employ the computational models to analyze previous experimental results and to explore the hypothesis that thematic fit, informed by generalized event knowledge, is ultimately responsible for the trigger of the logical metonymy. I then report on more psycholinguistic evidence showing that a notion of type is indeed necessary to account for differences between metonymic and non-metonymic constructions, and that both type and thematic fit play a role in logical metonymy interpretation. Lastly, I argue for a context-sensitive model of logical metonymy interpretation that exploits an information-rich lexicon, but needs to rethink the notion of type and reconcile it with the notion of thematic fit.","tags":["complement coercion","expectation-based processing","distributional semantics","logical metonymy","lexical meaning in context","self-paced reading","type clash","typicality","world knowledge"],"title":"Event Knowledge and Models of Logical Metonymy Interpretation","type":"publication"},{"authors":["Alessandra Zarcone","Sebastian Padó","Alessandro Lenci"],"categories":null,"content":"","date":1406073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1406073600,"objectID":"59c96aad48bacb2ed63f4965b3d5d0a2","permalink":"http://azarcone.github.io/publication/2014-cogsci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2014-cogsci/","section":"publication","summary":"We aim to model the results from a self-paced reading experiment, which tested the effect of semantic type clash and typicality on the processing of German complement coercion. We present two distributional semantic models to test if they can model the effect of both type and typicality in the psycholinguistic study. We show that one of the models, without explicitly representing type information, can account both for the effect of type and typicality in complement coercion.","tags":["complement coercion","distributional semantics","expectation-based processing","logical metonymy","lexical meaning in context","type clash","typicality","world knowledge"],"title":"Type and Thematic Fit in Logical Metonymy","type":"publication"},{"authors":["Alessandra Zarcone","Sebastian Padó","Alessandro Lenci"],"categories":null,"content":"","date":1394755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1394755200,"objectID":"d9fc85003ed8ffa2d22247ecd355c1e6","permalink":"http://azarcone.github.io/publication/2014-cogscij/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2014-cogscij/","section":"publication","summary":"Abstract Logical metonymy resolution (begin a book → begin reading a book or begin writing a book) has traditionally been explained either through complex lexical entries (qualia structures) or through the integration of the implicit event via post-lexical access to world knowledge. We propose that recent work within the words-as-cues paradigm can provide a more dynamic model of logical metonymy, accounting for early and dynamic integration of complex event information depending on previous contextual cues (agent and patient). We first present a self-paced reading experiment on German subordinate sentences, where metonymic sentences and their paraphrased version differ only in the presence or absence of the clause-final target verb (Der Konditor begann die Glasur → Der Konditor begann, die Glasur aufzutragen/The baker began the icing → The baker began spreading the icing). Longer reading times at the target verb position in a high-typicality condition (baker + icing → spread ) compared to a low-typicality (but still plausible) condition (child + icing → spread) suggest that we make use of knowledge activated by lexical cues to build expectations about events. The early and dynamic integration of event knowledge in metonymy interpretation is bolstered by further evidence from a second experiment using the probe recognition paradigm. Presenting covert events as probes following a high-typicality or a low-typicality metonymic sentence (Der Konditor begann die Glasur → AUFTRAGEN/The baker began the icing → SPREAD), we obtain an analogous effect of typicality at 100 ms interstimulus interval","tags":["complement coercion","expectation-based processing","logical metonymy","lexical meaning in context","self-paced reading","type clash","typicality","world knowledge"],"title":"Logical Metonymy Resolution in a Words-as-Cues Framework: Evidence From Self-Paced Reading and Probe Recognition","type":"publication"},{"authors":[],"categories":null,"content":"","date":1384081200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1384081200,"objectID":"8728f1e20063153819b09757af6785fe","permalink":"http://azarcone.github.io/talk/incrementality-in-compositional-distributional-semantics/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/incrementality-in-compositional-distributional-semantics/","section":"event","summary":"Invited contribution to a seminar on the role and expressiveness of distributional models of semantics and statistically derived models of language and linguistic behavior","tags":[],"title":"Incrementality in Compositional Distributional Semantics","type":"event"},{"authors":["Alessandra Zarcone","Sebastian Padó"],"categories":null,"content":"","date":1378080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1378080000,"objectID":"dcaff5db8dec7cef90a026a73779087f","permalink":"http://azarcone.github.io/publication/2014-amlap/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2014-amlap/","section":"publication","summary":"We aim to model the results from a self-paced reading experiment, which tested the effect of semantic type clash and typicality on the processing of German complement coercion. We present two distributional semantic models to test if they can model the effect of both type and typicality in the psycholinguistic study. We show that one of the models, without explicitly representing type information, can account both for the effect of type and typicality in complement coercion.","tags":["complement coercion","distributional semantics","expectation-based processing","logical metonymy","lexical meaning in context","type clash","typicality","world knowledge"],"title":"Logical metonymy: Disentangling object type and thematic fit","type":"publication"},{"authors":[],"categories":null,"content":"","date":1356087600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356087600,"objectID":"97dfe66bfbc23881b7dc416945b976cd","permalink":"http://azarcone.github.io/talk/dimmi-con-che-parole-vai-e-ti-diro-chi-sei-modelli-distribuzionali-del-significato-ed-esperimenti-psicolinguistici/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/dimmi-con-che-parole-vai-e-ti-diro-chi-sei-modelli-distribuzionali-del-significato-ed-esperimenti-psicolinguistici/","section":"event","summary":"Presentazione sui modelli distribuzionali del significato e sulla loro validazione psicolinguistica.","tags":[],"title":"Dimmi con che parole vai e ti dirò chi sei: modelli distribuzionali del significato ed esperimenti psicolinguistici","type":"event"},{"authors":[],"categories":null,"content":"","date":1354183200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1354183200,"objectID":"fc8bb9848c25b18f7307a43176d8c465","permalink":"http://azarcone.github.io/talk/logical-metonymy-resolution-in-a-words-as-cues-framework-beyond-type-clash/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/logical-metonymy-resolution-in-a-words-as-cues-framework-beyond-type-clash/","section":"event","summary":"Talk on complement coercion through the lens of the word-as-cues framework","tags":[],"title":"Logical metonymy resolution in a words-as-cues framework: beyond type-clash","type":"event"},{"authors":[],"categories":null,"content":"","date":1339509600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1339509600,"objectID":"f9d3029b34dd77a94276c6e442c611a9","permalink":"http://azarcone.github.io/talk/event-knowledge-and-typicality-in-logical-metonymy-beyond-type-clash/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/event-knowledge-and-typicality-in-logical-metonymy-beyond-type-clash/","section":"event","summary":"Talk on the plausibility-driven interpretation of covert events.","tags":[],"title":"Event knowledge and typicality in logical metonymy: beyond type clash","type":"event"},{"authors":[],"categories":null,"content":"","date":1323166500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1323166500,"objectID":"67754a6a738337eb782beb2215b39eb0","permalink":"http://azarcone.github.io/talk/metodi-di-crowdsourcing-nello-studio-del-linguaggio/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/metodi-di-crowdsourcing-nello-studio-del-linguaggio/","section":"event","summary":"Presentazione in occasione di un evento per i dottorandi in linguistica dell'Università di Pisa.","tags":[],"title":"Metodi di crowdsourcing nello studio del linguaggio","type":"event"},{"authors":["Alessandra Zarcone"],"categories":null,"content":"","date":1237248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1237248000,"objectID":"9aaf432bd3fbc85554711dd354104e99","permalink":"http://azarcone.github.io/publication/2009-mathesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2009-mathesis/","section":"publication","summary":"Event types (ET) have been widely addressed in linguistics literature, but have received little attention in psycholinguitics, neurolinguistics and computational linguistics research. This thesis dissertation explores the nature of event types from a cognitive point of view: many descriptions and diagnostics on event types are available, but few studies have dealt with the problem of how event types are represented and processed in the mental lexicon. An important prerequisite for this sort of research is the building of a corpus of stimuli that meets our needs (web-based pre-tests were run to test the reliability of the stimuli, which should be balanced to control the variables known to affect processing costs) and an analysis of pre-existing literature in experimental psycholinguistics of event types. Our main concern was to explore new experimental settings in verb semantics psycholinguistics and to adapt them to this specific type of investigation: the choice of the method was narrowed down to the semantic priming paradigm, although the set of stimuli could also be suitable for other experimental settings, such as reading-time studies. The semantic priming paradigm was exploited to contrast processing effects on achievement verbs and activity verbs, which differ with respect to two superordinate features: durativity and resultativity. A series of priming experiments were run to explore differences and interactions between such features and the tense morphology and to evaluate the different contribution of the experimental setting in the observation and measurement of the effect: experiment 1 and experiment 2 followed a similar design and contrasted the effects of different neutral primes; experiment 3 focused on the interaction between event types and Italian tense morphology","tags":["aspectual features","lexical aspect","lexical meaning in context","semantic priming"],"title":"Empirical correlates of event types - a priming study","type":"publication"},{"authors":["Alessandra Zarcone"],"categories":null,"content":"","date":1160611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1160611200,"objectID":"f8f3d73e73d07a48cff9bb9f992dc9c0","permalink":"http://azarcone.github.io/publication/2006-bscthesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2006-bscthesis/","section":"publication","summary":"La presente relazione illustra i primi risultati di un'analisi computazionale dell'azionalità del verbo italiano, seguendo l'ipotesi (presentata per l'inglese americano da Siegel e McKeown (2000)) che specifici indicatori contestuali presenti in una frase possano fornire delle informazioni rilevanti sulla categoria azionale del verbo della frase stessa. Un modello computazionale della classificazione azionale può rivelarsi particolarmente interessante per il trattamento di fenomeni come le opposizioni infralessicali e la commutazione azionale. Il modello di classificazione azionale adottato è quello introdotto da Zeno Vendler, con particolare attenzione anche all'elaborazione teorica alle problematiche specifiche del verbo italiano presentate da Pier Marco Bertinetto. Il modello sperimentale adottato è l'apprendimento automatico supervisionato detto Maximum Entropy (principio della massimizzazione dell'entropia - Berger et al. 1996). L'uso dell'apprendimento automatico può essere un modo innovativo di modellizzare il problema della classificazione azionale, ma può anche fornire consistenza sperimentale alle correlazioni teoriche tra marche contestuali e categorie azionali: la rilevanza delle diverse marche contestuali è valutata in base ai risultati ottenuti dall'agente in grado di apprendere, che, dopo una prima fase di addestramento, deve decidere circa la categoria azionale di una determinata forma verbale. Un analizzatore di questo tipo sfrutta soltanto elementi presenti nel contesto, senza l'ausilio di informazioni semantiche interne al verbo stesso, nè conoscenze afferenti al dominio della pragmatica, che spesso nell'uomo hanno un ruolo rilevante.","tags":["aspectual features","human upper bound","lexical aspect","lexical meaning in context","Maximum Enthropy"],"title":"La classificazione azionale del verbo italiano: primi esperimenti computazionali","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"http://azarcone.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]