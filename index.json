[{"authors":null,"categories":null,"content":"I am a computational linguist with a background in NLP and in psycholinguistics. I am interested in\n data-centered AI: how we can improve data quality to train better machine learning models on language tasks human-centered AI, that is how we can take humans (users, designers, crowdworkers and annotators) into account in all things AI to make the best of their intuitions and to interact with them in a fair way  ","date":1670371200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670371200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a computational linguist with a background in NLP and in psycholinguistics. I am interested in\n data-centered AI: how we can improve data quality to train better machine learning models on language tasks human-centered AI, that is how we can take humans (users, designers, crowdworkers and annotators) into account in all things AI to make the best of their intuitions and to interact with them in a fair way  ","tags":null,"title":"Alessandra Zarcone","type":"authors"},{"authors":null,"categories":null,"content":"Description  The workshop offers an immersive learning experience centered on Conversational AI, guiding participants through the process of developing their own chatbot and encouraging them to reflect on the issues related to human-machine interaction that arise when developing chatbots.\nParticipants   Students of MA Interactive Media Systems and European Project Semester (max 15) Language: English Prerequisite: Basic programming skills, interest in language-based human-machine interaction Bring your own laptop  When \u0026 Where  Monday, 4. December 2023 Tuesday, 5. December 2023 Friday, 8. December 2023\nTH Augsburg Master Interactive Media Systems\nPrerequisites   very basic programming knowledge (python) fascination with human-machine language interactions  -- Preliminary Schedule    Day 1Day 2Day 3   Morning (09:00 - 12:00) Theory: Intro to Conversational AI, Anatomy of a Dialogue AssistantTheory: Basics of Conversational DesignHands-on Work on your Dialogue Assistant, Intent and Entity Recognition  Afternoon (13:00 - 16:30) Practice Session: Implementing a Basic Chatbot, Choosing a Use Case and Simulating InteractionsPractice Session: Dialogue Management, Conversation FlowGroup Presentations: Presentation of Results, Lessons Learned      ","date":1689897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689897600,"objectID":"f4d4bc30b8b5d287f2b8b0747b511b25","permalink":"http://azarcone.github.io/teaching/ims-workshop-23/","publishdate":"2023-07-21T00:00:00Z","relpermalink":"/teaching/ims-workshop-23/","section":"teaching","summary":"IMS-ConvAI","tags":null,"title":"Workshop on Conversational AI (Winter Semester 2023/24)","type":"teaching"},{"authors":["Shima Asaadi","Zahra Kolagar","Alina Liebel","Alessandra Zarcone"],"categories":null,"content":"","date":1670371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670371200,"objectID":"717132e82422091b2c3ad0f95c6f27c1","permalink":"http://azarcone.github.io/publication/2022-gem/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2022-gem/","section":"publication","summary":"The Semantic textual similarity (STS) task is commonly used to evaluate the semantic representations that language models (LMs) learn from texts, under the assumption that good-quality representations will yield accurate similarity estimates. When it comes to estimating the similarity of two utterances in a dialogue, however, the conversational context plays a particularly important role. We argue for the need of benchmarks specifically created using conversational data in order to evaluate conversational LMs in the STS task. We introduce GiCCS, a first conversational STS evaluation benchmark for German. We collected the similarity annotations for GiCCS using best-worst scaling and presenting the target items in context, in order to obtain highly-reliable context-dependent similarity scores. We present benchmarking experiments for evaluating LMs on capturing the similarity of utterances. Results suggest that pretraining LMs on conversational data and providing conversational context can be useful for capturing similarity of utterances in dialogues. GiCCS is publicly available to encourage benchmarking of conversational LMs.","tags":["benchmark","crowdsourcing","conversational AI","data-centric","evaluation","semantic textual similarity","semantic similarity","NLU"],"title":"GiCCS: A German in-Context Conversational Similarity Benchmark","type":"publication"},{"authors":[],"categories":null,"content":"","date":1656428400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656428400,"objectID":"97cd506a4bb128c0e9c490723b58162e","permalink":"http://azarcone.github.io/talk/data-and-user-centric-nlp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/data-and-user-centric-nlp/","section":"event","summary":"Invited talk for the Internationalen Hochschule AI Impulse Serie","tags":[],"title":"Data- and user-centric NLP","type":"event"},{"authors":[],"categories":null,"content":"","date":1655298000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655298000,"objectID":"527e9d18177b1f1691d530e7e304cb22","permalink":"http://azarcone.github.io/talk/conversational-ai-between-hype-and-hope-a-case-for-data-and-human-centric-approaches/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/conversational-ai-between-hype-and-hope-a-case-for-data-and-human-centric-approaches/","section":"event","summary":"Invited talk for one of the weekly seminar at CLASP, University of Gothenburg.","tags":[],"title":"Conversational AI between hype and hope – A case for data- and human-centric approaches","type":"event"},{"authors":[],"categories":null,"content":"","date":1653505200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653505200,"objectID":"0a5e89b0ef60c5528c0d4d3d325999d0","permalink":"http://azarcone.github.io/talk/wie-alexa-sprechen-lernt-sprachassistenz-zwischen-datenhunger-und-datenschutz/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/wie-alexa-sprechen-lernt-sprachassistenz-zwischen-datenhunger-und-datenschutz/","section":"event","summary":"Die Vorlesung is teil einer großangelegten, öffentlichen Ringvorlesung der Hochschule Augsburg, die in zehn Veranstaltungen im Sommersemester 2022 die Auswirkungen Künstlicher Intelligenz auf unser gesellschaftliches Zusammenleben untersucht.","tags":[],"title":"Wie Alexa sprechen lernt – Sprachassistenz zwischen Datenhunger und Datenschutz","type":"event"},{"authors":["Lianna Hrycyk","Alessandra Zarcone","Luzian Hahn"],"categories":null,"content":"","date":1636588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636588800,"objectID":"00ea854d1453538d3a661dde8de9c6a5","permalink":"http://azarcone.github.io/publication/2021-nlp4convai/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-nlp4convai/","section":"publication","summary":"Incremental intent classification requires the assignment of intent labels to partial utterances. However, partial utterances do not necessarily contain enough information to be mapped to the intent class of their complete utterance (correctly and with a certain degree of confidence). Using the final interpretation as the ground truth to measure a classifier’s accuracy during intent classification of partial utterances is thus problematic. We release inCLINC, a dataset of partial and full utterances with human annotations of plausible intent labels for different portions of each utterance, as an upper (human) baseline for incremental intent classification. We analyse the incremental annotations and propose entropy reduction as a measure of human annotators’ convergence on an interpretation (i.e. intent label). We argue that, when the annotators do not converge to one or a few possible interpretations and yet the classifier already identifies the final intent class early on, it is a sign of overfitting that can be ascribed to artefacts in the dataset.","tags":["crowdsourcing","data-centric","human upper bound","incremental intent classification","incremental processing","Information Density","Information Theory","NLU"],"title":"Not So Fast, Classifier – Accuracy and Entropy Reduction in Incremental Intent Classification","type":"publication"},{"authors":["Alessandra Zarcone","Vera Demberg"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"7aade27170b204d4c6e4fd862ec84caa","permalink":"http://azarcone.github.io/publication/2021-cogsci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-cogsci/","section":"publication","summary":"The Uniform Information Density hypothesis (UID) predicts that lexical choice between long and short word forms depends on the predictability of the referent in context, and recent studies have shown such an effect of predictability on lexical choice during online production. We here set out to test whether the UID predictions hold up in a related setting, but different language (German) and different phenomenon, namely the choice between compounds (e.g. Badewanne / bathtub) or their base forms (Wanne / tub). Our study is consistent with the UID: we find that participants choose the shorter base form more often in predictive contexts, showing an active tendency to be information-theoretically efficient.","tags":["crowdsourcing","German compounds","expectation-based processing","incremental processing","Information Density","Information Theory","predictability","Rational Speech Act framework","referring expressions","script knowledge","Uniform Information Density hypothesis"],"title":"A bathtub by any other name: the reduction of German compounds in predictive contexts","type":"publication"},{"authors":["Alessandra Zarcone","Vera Demberg"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"0c497bd37c85ef6cf6491197e555640e","permalink":"http://azarcone.github.io/publication/2021-discproc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-discproc/","section":"publication","summary":"There is now a well-established literature showing that people anticipate upcoming concepts and words during language processing. Commonsense knowledge about typical event sequences and verbal selectional preferences can contribute to anticipating what will be mentioned next. We here investigate how temporal discourse connectives (before, after), which signal event ordering along a temporal dimension, modulate predictions for upcoming discourse referents. Our study analyses anticipatory gaze in the visual world and supports the idea that script knowledge, temporal connectives (before eating → menu, appetizer), and the verb’s selectional preferences (order → appetizer) jointly contribute to shaping rapid prediction of event participants.","tags":["discourse","expectation-based processing","incremental processing","script knowledge","world knowledge","visual world paradigm"],"title":"Interaction of Script Knowledge and Temporal Discourse Cues in a Visual World Study","type":"publication"},{"authors":["Yannick Frommherz","Alessandra Zarcone"],"categories":null,"content":"","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624233600,"objectID":"98ad4b6581ce18c8866a24e529317e25","permalink":"http://azarcone.github.io/publication/2021-frontcomputsci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-frontcomputsci/","section":"publication","summary":"Despite their increasing success, user interactions with smart speech assistants (SAs) are still very limited compared to human-human dialogue. One way to make SA interactions more natural is to train the underlying natural language processing modules on data which reflects how humans would talk to a SA if it was capable of understanding and producing natural dialogue given a specific task. Such data can be collected applying a Wizard-of-Oz approach (WOz), where user and system side are played by humans. WOz allows researchers to simulate human-machine interaction while benefitting from the fact that all participants are human and thus dialogue-competent. More recent approaches have leveraged simple templates specifying a dialogue scenario for crowdsourcing large-scale datasets. Template-based collection efforts, however, come at the cost of data diversity and naturalness. We present a method to crowdsource dialogue data for the SA domain in the WOz framework, which aims at limiting researcher-induced bias in the data while still allowing for a low-resource, scalable data collection. Our method can also be applied to languages other than English (in our case German), for which fewer crowd-workers may be available. We collected data asynchronously, relying only on existing functionalities of Amazon Mechanical Turk, by formulating the task as a dialogue continuation task. Coherence in dialogues is ensured, as crowd-workers always read the dialogue history, and as a unifying scenario is provided for each dialogue. In order to limit bias in the data, rather than using template-based scenarios, we handcrafted situated scenarios which aimed at not pre-script-ing the task into every single detail and not priming the participants’ lexical choices. Our scenarios cued people’s knowledge of common situations and entities relevant for our task, without directly mentioning them, but relying on vague language and circumlocutions. We compare our data (which we publish as the CROWDSS corpus; n = 113 dialogues) with data from MultiWOZ, showing that our scenario approach led to considerably less scripting and priming and thus more ecologically-valid dialogue data. This suggests that small investments in the collection setup can go a long way in improving data quality, even in a low-resource setup.","tags":["crowdsourcing","data-centric","dialogue","ecological data","Human-Machine Interaction","Wizard-of-Oz"],"title":"Crowdsourcing ecologically-valid dialogue data for German","type":"publication"},{"authors":["Yannick Frommherz","Alessandra Zarcone"],"categories":null,"content":"","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624233600,"objectID":"648227bc2a2b7610b5f5c303cbe5d028","permalink":"http://azarcone.github.io/dataset/2021-crowdss/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/dataset/2021-crowdss/","section":"dataset","summary":"Despite their increasing success, user interactions with smart speech assistants (SAs) are still very limited compared to human-human dialogue. One way to make SA interactions more natural is to train the underlying natural language processing modules on data which reflects how humans would talk to a SA if it was capable of understanding and producing natural dialogue given a specific task. Such data can be collected applying a Wizard-of-Oz approach (WOz), where user and system side are played by humans. WOz allows researchers to simulate human-machine interaction while benefitting from the fact that all participants are human and thus dialogue-competent. More recent approaches have leveraged simple templates specifying a dialogue scenario for crowdsourcing large-scale datasets. Template-based collection efforts, however, come at the cost of data diversity and naturalness. We present a method to crowdsource dialogue data for the SA domain in the WOz framework, which aims at limiting researcher-induced bias in the data while still allowing for a low-resource, scalable data collection. Our method can also be applied to languages other than English (in our case German), for which fewer crowd-workers may be available. We collected data asynchronously, relying only on existing functionalities of Amazon Mechanical Turk, by formulating the task as a dialogue continuation task. Coherence in dialogues is ensured, as crowd-workers always read the dialogue history, and as a unifying scenario is provided for each dialogue. In order to limit bias in the data, rather than using template-based scenarios, we handcrafted situated scenarios which aimed at not pre-script-ing the task into every single detail and not priming the participants’ lexical choices. Our scenarios cued people’s knowledge of common situations and entities relevant for our task, without directly mentioning them, but relying on vague language and circumlocutions. We compare our data (which we publish as the CROWDSS corpus; n = 113 dialogues) with data from MultiWOZ, showing that our scenario approach led to considerably less scripting and priming and thus more ecologically-valid dialogue data. This suggests that small investments in the collection setup can go a long way in improving data quality, even in a low-resource setup.","tags":["crowdsourcing","data-centric","dialogue","ecological data","Human-Machine Interaction","Wizard-of-Oz"],"title":"CROWDSS: A crowdsourced, ecologically-valid dialogue dataset for German","type":"dataset"},{"authors":["Lianna Hrycyk","Alessandra Zarcone","Luzian Hahn"],"categories":null,"content":"","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624233600,"objectID":"eb2452abf20e397a94ae726c43b316bf","permalink":"http://azarcone.github.io/dataset/2021-inclinc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/dataset/2021-inclinc/","section":"dataset","summary":"Despite their increasing success, user interactions with smart speech assistants (SAs) are still very limited compared to human-human dialogue. One way to make SA interactions more natural is to train the underlying natural language processing modules on data which reflects how humans would talk to a SA if it was capable of understanding and producing natural dialogue given a specific task. Such data can be collected applying a Wizard-of-Oz approach (WOz), where user and system side are played by humans. WOz allows researchers to simulate human-machine interaction while benefitting from the fact that all participants are human and thus dialogue-competent. More recent approaches have leveraged simple templates specifying a dialogue scenario for crowdsourcing large-scale datasets. Template-based collection efforts, however, come at the cost of data diversity and naturalness. We present a method to crowdsource dialogue data for the SA domain in the WOz framework, which aims at limiting researcher-induced bias in the data while still allowing for a low-resource, scalable data collection. Our method can also be applied to languages other than English (in our case German), for which fewer crowd-workers may be available. We collected data asynchronously, relying only on existing functionalities of Amazon Mechanical Turk, by formulating the task as a dialogue continuation task. Coherence in dialogues is ensured, as crowd-workers always read the dialogue history, and as a unifying scenario is provided for each dialogue. In order to limit bias in the data, rather than using template-based scenarios, we handcrafted situated scenarios which aimed at not pre-script-ing the task into every single detail and not priming the participants’ lexical choices. Our scenarios cued people’s knowledge of common situations and entities relevant for our task, without directly mentioning them, but relying on vague language and circumlocutions. We compare our data (which we publish as the CROWDSS corpus; n = 113 dialogues) with data from MultiWOZ, showing that our scenario approach led to considerably less scripting and priming and thus more ecologically-valid dialogue data. This suggests that small investments in the collection setup can go a long way in improving data quality, even in a low-resource setup.","tags":["crowdsourcing","data-centric","dialogue","ecological data","Human-Machine Interaction","Wizard-of-Oz"],"title":"inCLINC: incremental intent annotations of the CLINC dataset","type":"dataset"},{"authors":["Touhidul Alam","Alessandra Zarcone","Sebastian Padó"],"categories":null,"content":"","date":1623801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623801600,"objectID":"6b5cbedde421154443fcb6456970bf79","permalink":"http://azarcone.github.io/publication/2021-iwcs/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2021-iwcs/","section":"publication","summary":"Reliable tagging of Temporal Expressions (TEs, e.g., Book a table at L’Osteria for Sunday evening) is a central requirement for Voice Assistants (VAs). However, there is a dearth of resources and systems for the VA domain, since publicly-available temporal taggers are trained only on substantially different domains, such as news and clinical text. Since the cost of annotating large datasets is prohibitive, we investigate the trade-off between in-domain data and performance in DA-Time, a hybrid temporal tagger for the English VA domain which combines a neural architecture for robust TE recognition, with a parser-based TE normalizer. We find that transfer learning goes a long way even with as little as 25 in-domain sentences: DA-Time performs at the state of the art on the news domain, and substantially outperforms it on the VA domain.","tags":["domain adaptation","NLU","temporal expressions","temporal tagging","transfer learning","voice assistants"],"title":"New Domain, Major Effort? How Much Data is Necessary to Adapt a Temporal Tagger to the Voice Assistant Domain","type":"publication"},{"authors":[],"categories":null,"content":"","date":1619010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619010000,"objectID":"c37fb9cd8c648c0f21f91bbe49b28bdc","permalink":"http://azarcone.github.io/talk/hey-google-wie-sorgt-man-fur-gendergerechtigtkeit-in-der-maschinellen-sprachverarbeitung/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/hey-google-wie-sorgt-man-fur-gendergerechtigtkeit-in-der-maschinellen-sprachverarbeitung/","section":"event","summary":"Wir werden in diesem Vortrag gemeinsam aufdecken, welche Verzerrungen in datenbasierten Modellen der Sprache auftauchen können und wie man dagegen vorgehen kann.","tags":[],"title":"Hey Google, wie sorgt man für Gendergerechtigtkeit in der Maschinellen Sprachverarbeitung?","type":"event"},{"authors":["Alessandra Zarcone","吳恩達"],"categories":["Demo","教程"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"http://azarcone.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Alessandra Zarcone","Touhidul Alam","Zahra Kolagar"],"categories":null,"content":"","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"79ca6ebd7012e0bf844fe76c195a804c","permalink":"http://azarcone.github.io/publication/2020-lrec/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2020-lrec/","section":"publication","summary":"The recognition and automatic annotation of temporal expressions (e.g. “Add an event for tomorrow evening at eight to my calendar”) is a key module for AI voice assistants, in order to allow them to interact with apps (for example, a calendar app). However, in the NLP literature, research on temporal expressions has focused mostly on data from the news, from the clinical domain, and from social media. The voice assistant domain is very different than the typical domains that have been the focus of work on temporal expression identification, thus requiring a dedicated data collection. We present a crowdsourcing method for eliciting natural-language commands containing temporal expressions for an AI voice assistant, by using pictures and scenario descriptions. We annotated the elicited commands (480) as well as the commands in the Snips dataset following the TimeML/TIMEX3 annotation guidelines, reaching a total of 1188 annotated commands. The commands can be later used to train the NLU components of an AI voice assistant.","tags":["NLU","temporal expressions","temporal tagging","voice assistants"],"title":"PÂTÉ: A Corpus of Temporal Expressions for the In-car Voice Assistant Domain","type":"publication"},{"authors":["Alessandra Zarcone","Ken McRae","Alessandro Lenci","Sebastian Padó"],"categories":null,"content":"","date":1511481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511481600,"objectID":"611d3bd65ef34a83ec9ef3121f90a5cb","permalink":"http://azarcone.github.io/publication/2017-front-psychol/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2017-front-psychol/","section":"publication","summary":"Complement coercion (begin a book → reading) involves a type clash between an event-selecting verb and an entity-denoting object, triggering a covert event (reading). Two main factors involved in complement coercion have been investigated: the semantic type of the object (event vs. entity), and the typicality of the covert event (the author began a book → writing). In previous research, reading times have been measured at the object. However, the influence of the typicality of the subject–object combination on processing an aspectual verb such as begin has not been studied. Using a self-paced reading study, we manipulated semantic type and subject–object typicality, exploiting German word order to measure reading times at the aspectual verb. These variables interacted at the target verb. We conclude that both type and typicality probabilistically guide expectations about upcoming input. These results are compatible with an expectation-based view of complement coercion and language comprehension more generally in which there is rapid interaction between what is typically viewed as linguistic knowledge, and what is typically viewed as domain general knowledge about how the world works.","tags":["complement coercion","expectation-based processing","logical metonymy","lexical meaning in context","self-paced reading","type clash","typicality","world knowledge"],"title":"Complement Coercion: The Joint Effects of Type and Typicality","type":"publication"},{"authors":["Lilian D. A. Wanzare","Alessandra Zarcone","Stefan Thater","Manfred Pinkal"],"categories":null,"content":"","date":1491177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491177600,"objectID":"a789df0360a28909496b1ac1685838a0","permalink":"http://azarcone.github.io/publication/2017-lsdsem/","publishdate":"2017-04-03T00:00:00Z","relpermalink":"/publication/2017-lsdsem/","section":"publication","summary":"We present a semi-supervised clustering approach to induce script structure from crowdsourced descriptions of event sequences by grouping event descriptions into paraphrase sets (representing event types) and inducing their temporal order. Our approach exploits semantic and positional similarity and allows for flexible event order, thus overcoming the rigidity of previous approaches. We incorporate crowdsourced alignments as prior knowledge and show that exploiting a small number of alignments results in a substantial improvement in cluster quality over state-of-the-art models and provides an appropriate basis for the induction of temporal order. We also show a coverage study to demonstrate the scalability of our approach.","tags":["clustering","crowdsourcing","event order","script knowledge"],"title":"Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering","type":"publication"},{"authors":["Alessandra Zarcone","Vera Demberg"],"categories":null,"content":"","date":1490832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490832000,"objectID":"ebc790530aa141eb122e8946bd2de93d","permalink":"http://azarcone.github.io/publication/2017-cuny/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2017-cuny/","section":"publication","summary":"Habitual knowledge of everyday activities (e.g. going to a restaurant) plays a key role in our interaction with the environment and in language comprehension. Such events and their participants are stored in our semantic memory (as script knowledge) in an order-sensitive way: the order of mention of script events has been shown to determine if following events are cued (e.g. cook, sit → dine). Language also uses discourse connectors, and temporal ones in particular (before / after), to signal possible reordering of the events. We then assume that temporal connectives trigger expectations about the events’ order of mention, generating predictions about what event will be mentioned next. According to the iconicity assumption, processing is easier when the order of mention matches the chronological order of the events, as no reordering is needed. Previous work has shown the effect of fast integration of discourse content and connectives, but the early integration of discourse connectives and script knowledge has not been shown before. Our hypothesis is that people make an early use not only of discourse connectives but also of broader world knowledge information about scripts as soon as context cues become available, to quickly build expectations about what will be mentioned next.\nIn the present visual world study participants heard short stories, each about a particular scenario, followed by test sentences such as: (a) Before / After the meal read he eagerly the menu / the bill (b) Before / After the meal ordered he eagerly an appetizer / a dessert. Along with the sentences, participants are presented with visual scenes, each containing six objects: one target object (e.g. menu), one temporal-order distractor, which holds the same temporal order relation (before or after) with the main event as the target (e.g. appetizer, referring to another before-event), one selectional-restriction distractor, matching the same main verb as the target (e.g. bill), and two fillers, which were congruent with the script, but did not match the verb nor were participants in an obvious before- or after-event. We analyze three regions: the before/after region (starting 200 ms into the main event, e.g. dem Essen), the verb region (starting 200 ms into the verb onset, e.g. las), and the object region (starting 200 ms into the object’s determiner, e.g. die Speisekarte). At the before/after region, we expect the connectors to cue the objects matching the participant’s script knowledge of what event comes before or after the main event, resulting in more gazes at the target and at the temporal order distractor compared to the other objects. Following the iconicity assumption, we expect the effect of the discourse connector to be stronger for the after condition compared to the before condition. At the verb region, we expect the verb’s selectional restrictions to interact with the temporal discourse cues, restricting the comprehender's prediction to the target object, that is the only object matching both the temporal order cues and the verb’s selectional restrictions.\nPreliminary results (N = 16) show an increased proportion of first fixations on the objects matching the temporal order (target + temporal-order distractor) at the before/after region for the after condition, but not for the before condition. First fixations on the objects matching the verb (target + selectional-restriction distractor) are increased at the verb region for both conditions. The event order cues and the verbal cues are integrated as early as the verb region, where we observe overall increased first fixations on the target. Unsurprisingly, first fixations on the target are also increased at the object. The advantage of the after-condition shows that people have a natural tendency to anticipate the next script event before it is mentioned, rather than the previous one, even when this is cued by context. Our results are compatible with expectation-based views of language processing, as they show that people quickly integrate linguistic cues (relative to the verb’s preferences) and world knowledge about habitual events and their typical order as soon as such cues become available.","tags":["discourse","expectation-based processing","incremental processing","script knowledge","visual world paradigm","world knowledge"],"title":"Script knowledge and event order in a visual world paradigm study","type":"publication"},{"authors":["Alessandra Zarcone","Marten van Schijndel","Jorrig Vogels","Vera Demberg"],"categories":null,"content":"","date":1465171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465171200,"objectID":"7085f1fdc43e7111d35a84e7d8915004","permalink":"http://azarcone.github.io/publication/2016-front-psychol/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2016-front-psychol/","section":"publication","summary":"The notion of salience has been singled out as the explanatory factor for a diverse range of linguistic phenomena. In particular, perceptual salience (e.g., visual salience of objects in the world, acoustic prominence of linguistic sounds) and semantic-pragmatic salience (e.g., prominence of recently mentioned or topical referents) have been shown to influence language comprehension and production. A different line of research has sought to account for behavioral correlates of cognitive load during comprehension as well as for certain patterns in language usage using information-theoretic notions, such as surprisal. Surprisal and salience both affect language processing at different levels, but the relationship between the two has not been adequately elucidated, and the question of whether salience can be reduced to surprisal / predictability is still open. Our review identifies two main challenges in addressing this question: terminological inconsistency and lack of integration between high and low levels of representations in salience-based accounts and surprisal-based accounts. We capitalize upon work in visual cognition in order to orient ourselves in surveying the different facets of the notion of salience in linguistics and their relation with models of surprisal. We find that work on salience highlights aspects of linguistic communication that models of surprisal tend to overlook, namely the role of attention and relevance to current goals, and we argue that the Predictive Coding framework provides a unified view which can account for the role played by attention and predictability at different levels of processing and which can clarify the interplay between low and high levels of processes and between predictability-driven expectation and attention-driven focus.","tags":["attention","expectation-based processing","Information Theory","predictive coding","perceptual salience","predictability","salience","surprisal"],"title":"Salience and Attention in Surprisal-Based Accounts of Language Processing","type":"publication"},{"authors":["Lilian D. A. Wanzare","Alessandra Zarcone","Stefan Thater","Manfred Pinkal"],"categories":null,"content":"","date":1463961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463961600,"objectID":"ff2bc4bd6184b2a13dd384f53a816ad9","permalink":"http://azarcone.github.io/publication/2016-lrec/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2016-lrec/","section":"publication","summary":"Scripts are standardized event sequences describing typical everyday activities, which play an important role in the computational modeling of cognitive abilities (in particular for natural language processing). We present a large-scale crowdsourced collection of explicit linguistic descriptions of script-specific event sequences (40 scenarios with 100 sequences each). The corpus is enriched with crowdsourced alignment annotation on a subset of the event descriptions, to be used in future work as seed data for automatic alignment of event descriptions (for example via clustering). The event descriptions to be aligned were chosen among those expected to have the strongest corrective effect on the clustering algorithm. The alignment annotation was evaluated against a gold standard of expert annotators. The resulting database of partially-aligned script-event descriptions provides a sound empirical basis for inducing high-quality script knowledge, as well as for any task involving alignment and paraphrase detection of events","tags":["crowdsourcing","event order","script knowledge"],"title":"DeScript: A Crowdsourced Corpus for the Acquisition of High-Quality Script Knowledge","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"http://azarcone.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"Principal Investigator: Sebastian Padó, Sabine Schulte im WaldeResearchers: Alessandra Zarcone, Jason UttAssociated Ph.D. Student: Gabriella Lapesa \nProject D6 was concerned with a particular phenomenon of incremental specification in context, namely the interpretation of verb-object pairs. While some verb-object pairs are interpreted compositionally (\"simple composition\", as in begin a holiday), the understanding of others involves covert events that are not realised on the surface (\"enriched composition\": for example, begin a song, in neutral context, implies begin singing a song).\nThe interpretation of such pairs involves two specification steps:(1) the decision between simple and enriched composition;(2) in enriched composition, the specification of the covert event.\nThe project used psycholinguistic and computational techniques to elucidate the as yet neglected role of lexical-semantic context factors. Concretely, the literature about step (1) assumes that the ontological type of the object (event/object) is the main determinant. Since this is arguably an oversimplification, it collected and modeled experimental data on:(a) the behaviour of ambiguous nominalisations and nouns;(b) the respective influence of ontological type and plausibility;(c) the role played by aspectual features.\nIn its turn, step (2) was treated by previous work as merely the choice of a single most plausible event. In contrast, the project elicited a range of covert events for each instance of enriched composition (for begin a song, this might be sing, chant, and croon), and analysed what semantic relations hold within this set. This analysis was used to model the interpretations in terms of concepts, to be approximated by verb clusters. The interpretation process will be analysed in parallel for English and Italian.\nA practical result of our studies was a framework for distributional corpus-driven, broad-coverage computational models that predicts both the simple/enriched composition distinction for sentences potentially involving covert events, and, in the case of enriched composition, the range of interpretations appropriate for the present context.\nFinally, it extended the scope of our core results in three ways.(1) We phrased the recovery of covert event interpretations as an entailment task, to make our phenomenon accessible to a larger audience in computational semantics;(2) We studied a related phenomenon, namely Italian da-nominals, which also imply covert events, e.g.scatola da scarpe / shoe box -\u0026gt; box to store shoes;(3) We developed a distributional-driven characterisation of constructions whose interpretation involves covert events.\nThe Metonymy 2011 workshop (Stuttgart, 15th-16th September 2011) was part of the project activities. ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"a8175f0d968df85837ba702cffa6a132","permalink":"http://azarcone.github.io/project/d6/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/d6/","section":"project","summary":"D6","tags":["Complement Coercion","Generalized Event Knowledge","Logical Metonymy","SFB732"],"title":"Lexical-semantic factors in event interpretation","type":"project"},{"authors":["Alessandra Zarcone","Sebastian Padó","Alessandro Lenci"],"categories":null,"content":"","date":1427673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1427673600,"objectID":"1f806f1b8184bc425e784ac5e0744cb4","permalink":"http://azarcone.github.io/publication/2015-networds/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2015-networds/","section":"publication","summary":"We aim to model the results from a self-paced reading experiment, which tested the effect of semantic type clash and typicality on the processing of German complement coercion. We present two distributional semantic models to test if they can model the effect of both type and typicality in the psycholinguistic study. We show that one of the models, without explicitly representing type information, can account both for the effect of type and typicality in complement coercion.","tags":["complement coercion","distributional semantics","expectation-based processing","logical metonymy","lexical meaning in context","type clash","typicality","world knowledge"],"title":"Same same but different: Type and typicality in a distributional model of complement coercion","type":"publication"},{"authors":["Olga Batiukova","Pier Marco Bertinetto","Alessandro Lenci","Alessandra Zarcone"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"7f062f1669068944bec9e53bbda60730","permalink":"http://azarcone.github.io/publication/2015-cahierschronos/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2015-cahierschronos/","section":"publication","summary":"This paper reports four semantic priming experiments in Italian and Spanish, whose goal was to verify the psychological reality of two aspectual features, resultativity and durativity. In the durativity task, the participants were asked whether the verb referred to a durable situation, in the resultativity task if it denoted a situation with a clear outcome. The results prove that both features are involved in online processing of the verb meaning: achievements ([+resultative, -durative]) and activities ([-resultative, +durative]) were processed faster in certain priming contexts. The priming patterns in the Romance languages present some striking similarities (only achievements were primed in the resultativity task) alongside some intriguing differences, and interestingly contrast with the behaviour of another language tested, Russian, whose aspectual system differs in significant ways.","tags":["aspectual features","lexical aspect","lexical meaning in context","semantic priming"],"title":"Identifying actional features through semantic priming: Cross-Romance comparison","type":"publication"},{"authors":["Alessandra Zarcone"],"categories":null,"content":"","date":1409529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409529600,"objectID":"e57de740aa79df6dc59d69a6d796207e","permalink":"http://azarcone.github.io/publication/2014-phdthesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2014-phdthesis/","section":"publication","summary":"During language understanding, people do not only rely on what they read or hear, but they also exploit implicit information. For example, when they process the expression 'begin the book', they understand it involves an event which is not explicitly mentioned (e.g. 'begin reading the book'). This thesis looks at these constructions, known as logical metonymies, which combine an event-selecting verb and entity-denoting object and involve covert events. Logical metonymies are an interesting challenge for theories of lexical semantics: they need to be reconciled with compositionality, they require the integration of context (writers typically write books, students typically read them), and they lie at the interface between lexicon and world knowledge (is the information that books are read stored in our mental lexicon or in our world knowledge?).\nI critically analyze previous hypotheses on logical metonymy with regard to the answer they provide to two core problems: the source problem (what events are retrieved? what type of event knowledge is assumed?) and the trigger problem (why do some constructions trigger a metonymic interpretation and others do not?). Lexicalist approaches claim that the metonymy arises from a type clash between the event-selecting verb and an entity-denoting object, and posit complex lexical items, encoding event information about artifacts (e.g. book → read), to explain the recovery of covert events. Pragmatic-based approaches argue against the idea that lexical items have an internal structure, suggesting that covert events arise from the underspecification of a logical metonymy and are inferred via non-lexical knowledge. I look with particular attention at the role of event knowledge, which lexicalist approaches place in our mental lexicon, while pragmatic-based approaches place it in our world knowledge.\nI propose a third hypothesis, based on thematic fit and generalized event knowledge of typical events and their participants, which have been shown to guide efficient incremental processing: I argue that contextual elements cue generalized event knowledge, which plays a key role in determining the covert event for a logical metonymy. I explore this hypothesis from an interdisciplinary perspective, employing both psycholinguistic experiments and computational models, in order to seek converging evidence and confront it with the theoretical investigation. The results from the psycholinguistic experiments and from the computational (distributional) models support the hypothesis that covert event retrieval is guided by generalized event knowledge. I also employ the computational models to analyze previous experimental results and to explore the hypothesis that thematic fit, informed by generalized event knowledge, is ultimately responsible for the trigger of the logical metonymy. I then report on more psycholinguistic evidence showing that a notion of type is indeed necessary to account for differences between metonymic and non-metonymic constructions, and that both type and thematic fit play a role in logical metonymy interpretation. Lastly, I argue for a context-sensitive model of logical metonymy interpretation that exploits an information-rich lexicon, but needs to rethink the notion of type and reconcile it with the notion of thematic fit.","tags":["complement coercion","expectation-based processing","distributional semantics","logical metonymy","lexical meaning in context","self-paced reading","type clash","typicality","world knowledge"],"title":"Event Knowledge and Models of Logical Metonymy Interpretation","type":"publication"},{"authors":["Alessandra Zarcone","Sebastian Padó","Alessandro Lenci"],"categories":null,"content":"","date":1406073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1406073600,"objectID":"59c96aad48bacb2ed63f4965b3d5d0a2","permalink":"http://azarcone.github.io/publication/2014-cogsci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2014-cogsci/","section":"publication","summary":"We aim to model the results from a self-paced reading experiment, which tested the effect of semantic type clash and typicality on the processing of German complement coercion. We present two distributional semantic models to test if they can model the effect of both type and typicality in the psycholinguistic study. We show that one of the models, without explicitly representing type information, can account both for the effect of type and typicality in complement coercion.","tags":["complement coercion","distributional semantics","expectation-based processing","logical metonymy","lexical meaning in context","type clash","typicality","world knowledge"],"title":"Type and Thematic Fit in Logical Metonymy","type":"publication"},{"authors":["Olga Batiukova","Pier Marco Bertinetto","Alessandro Lenci","Alessandra Zarcone","Irene Ricci"],"categories":null,"content":"","date":1402876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1402876800,"objectID":"ed2f3d1ad8d7327aad4e75bb226d2934","permalink":"http://azarcone.github.io/publication/2014-chronos/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2014-chronos/","section":"publication","summary":"We report and contrast the results of a series of priming studies in Italian and Spanish, whose main goal was to empirically verify the psychological reality of two features crucially involved in event type classification: resultativity and durativity. Unlike most experimental research, focused on ontological properties of actional classes of predicates (Todorova et al. 2000, Gennari \u0026 Poeppel 2003, Pylkkänen \u0026 McElree 2006, Bott 2010), we adopt a feature-based approach to verbal semantics. Semantic priming is one of the most common experimental paradigms to probe the organization of the lexicon (McNamara 2005). We apply it here to get behavioral data about the role of event type features of verb meaning. We followed the general design in Bonnotte (2008), who tested French achievement and activity verbs (respectively classified as [+resultative, -durative] and [-resultative, +durative]) in two semantic decision tasks: the subjects were asked if the target referred to a durable or non-durable situation and whether it implied a clear outcome or not. Our Italian and Spanish semantic decision experiments presented a number of modifications in data selection with respect to Bonnotte’s design. First of all, we ran an event classification pre-test to make sure that only the most unambiguously identified achievements and activities were included in the data set. Further modifications were introduced to minimize any semantic effect not related to event types (the semantic relatedness of primes and targets was measured in a web-based pre-test), and to protect the actional classes tested from any factor that might blur the semantic interpretation (punctual verbs, which are often confused with achievements, were excluded from the data set). A total of 36 verbs (18 activities and 18 achievements) were selected for the final data set. The pattern of priming effects obtained for the Romance languages presents some striking similarities: in the resultativity task, only achievements benefited from priming from opposite primes. There are, however, some intriguing differences, too: in the durativity task, activities were processed significantly faster after similar primes in Italian but in Spanish only achievements were primed. In addition to the semantic decision task, we ran a lexical decision task to check whether both features impact the processing even when the speakers do not have to consciously identify the verbs marked with these features. In other words, we wanted to check whether these features exert their effect in an on-line task, in an automatic and implicit way. The results obtained for Spanish seem to confirm that this is indeed the case: achievements benefited from opposite primes and activities from similar primes. This replicates exactly the pattern observed in the semantic decision task for Italian and, in part, for Spanish. Data collection for Italian verbs with a similar lexical decision task is currently ongoing. This seems to suggest that the durativity and resultativity features can be selectively activated in the mental representation of verbal semantics, bringing further empirical support to the feature-based approach to event types.","tags":["aspectual features","lexical aspect","lexical meaning in context","semantic priming"],"title":"Priming-based study of durativity and resultativity in Spanish and Italian","type":"publication"},{"authors":["Alessandra Zarcone","Sebastian Padó","Alessandro Lenci"],"categories":null,"content":"","date":1394755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1394755200,"objectID":"d9fc85003ed8ffa2d22247ecd355c1e6","permalink":"http://azarcone.github.io/publication/2014-cogscij/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2014-cogscij/","section":"publication","summary":"Abstract Logical metonymy resolution (begin a book → begin reading a book or begin writing a book) has traditionally been explained either through complex lexical entries (qualia structures) or through the integration of the implicit event via post-lexical access to world knowledge. We propose that recent work within the words-as-cues paradigm can provide a more dynamic model of logical metonymy, accounting for early and dynamic integration of complex event information depending on previous contextual cues (agent and patient). We first present a self-paced reading experiment on German subordinate sentences, where metonymic sentences and their paraphrased version differ only in the presence or absence of the clause-final target verb (Der Konditor begann die Glasur → Der Konditor begann, die Glasur aufzutragen/The baker began the icing → The baker began spreading the icing). Longer reading times at the target verb position in a high-typicality condition (baker + icing → spread ) compared to a low-typicality (but still plausible) condition (child + icing → spread) suggest that we make use of knowledge activated by lexical cues to build expectations about events. The early and dynamic integration of event knowledge in metonymy interpretation is bolstered by further evidence from a second experiment using the probe recognition paradigm. Presenting covert events as probes following a high-typicality or a low-typicality metonymic sentence (Der Konditor begann die Glasur → AUFTRAGEN/The baker began the icing → SPREAD), we obtain an analogous effect of typicality at 100 ms interstimulus interval","tags":["complement coercion","expectation-based processing","logical metonymy","lexical meaning in context","self-paced reading","type clash","typicality","world knowledge"],"title":"Logical Metonymy Resolution in a Words-as-Cues Framework: Evidence From Self-Paced Reading and Probe Recognition","type":"publication"},{"authors":[],"categories":null,"content":"","date":1384081200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1384081200,"objectID":"8728f1e20063153819b09757af6785fe","permalink":"http://azarcone.github.io/talk/incrementality-in-compositional-distributional-semantics/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/incrementality-in-compositional-distributional-semantics/","section":"event","summary":"Invited contribution to a seminar on the role and expressiveness of distributional models of semantics and statistically derived models of language and linguistic behavior","tags":[],"title":"Incrementality in Compositional Distributional Semantics","type":"event"},{"authors":["Alessandra Zarcone","Sebastian Padó"],"categories":null,"content":"","date":1378080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1378080000,"objectID":"dcaff5db8dec7cef90a026a73779087f","permalink":"http://azarcone.github.io/publication/2014-amlap/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2014-amlap/","section":"publication","summary":"We aim to model the results from a self-paced reading experiment, which tested the effect of semantic type clash and typicality on the processing of German complement coercion. We present two distributional semantic models to test if they can model the effect of both type and typicality in the psycholinguistic study. We show that one of the models, without explicitly representing type information, can account both for the effect of type and typicality in complement coercion.","tags":["complement coercion","distributional semantics","expectation-based processing","logical metonymy","lexical meaning in context","type clash","typicality","world knowledge"],"title":"Logical metonymy: Disentangling object type and thematic fit","type":"publication"},{"authors":["Alessandra Zarcone"],"categories":null,"content":"","date":1371686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1371686400,"objectID":"ee45a7278314c87631baaed822264c4d","permalink":"http://azarcone.github.io/publication/2013-detec/","publishdate":"2013-06-20T00:00:00Z","relpermalink":"/publication/2013-detec/","section":"publication","summary":"Logical metonymies (e.g. the student began the book) have often been treated as a case of type-clash (Pustejovsky 1995): an event-subcategorizing verb (begin) is combined with an entity-denoting object (the book), leading to (1) extra processing costs, ascribed to coercion, a compositional operation needed to construct an event sense for the object (Frisson and McElree 2008) and (2) the recovery of a covert event from complex lexical entries (object → reading, writing). Nevertheless, Lascarides and Copestake (1998) have observed that lexical information is not enough to account for the range of interpretations in logical metonymy (for example, to retrieve the interpretation for My goats eat anything. He really enjoyed your book). They claim that pragmatic inference is often needed to retrieve discourse-relevant interpretations, and thus advocate for an interaction between lexicon and pragmatics. While focusing on the role of type clash, previous experimental studies on logical metonymy have only marginally considered effects of discourse in logical metonymy interpretation (but see de Almeida and Dwivedi 2008) and their interaction with type. Selectional preferences are shaped by generalized event knowledge (Matsuki et al. 2011), which can be described in terms of thematic fit, that is, the typicality of a filler for an argument slot (e.g., the fact that eat requires a [+edible] object or that thief is a more fitting object for arrest than policeman). Thematic fit can be influenced by inter-sentential context but also by a wider discourse context; psycholinguistic studies have shown how people make extensive use of knowledge of typical scenarios to exploit contextual cues and build expectations about upcoming input in language (that is, input with the highest thematic fit with previous context, McRae et al. 1998; Matsuki et al. 2011). We propose a first step towards an expectation-based account of logical metonymy interpretation, where processing costs for logical metonymies are modulated by discourse-driven expectations about upcoming input. We suggest that thematic fit plays an important role in logical metonymy interpretation, by (1) distinguishing metonymic contexts (begin has a low thematic fit for entity-denoting objects such as book, making book a less expected object for begin than an event-denoting object), and (2) determining the most expected interpretation (the interpretation with the highest thematic fit with the context: the author began the book → writing, the student began the book → reading, Zarcone and Padó 2011).","tags":["Complement Coercion","expectation-based processing","Logical Metonymy","lexical meaning in context","type clash","typicality","world knowledge"],"title":"An expectation-based account of logical metonymy interpretation","type":"publication"},{"authors":["Alessandra Zarcone","Alessandro Lenci","Sebastian Padó","Jason Utt"],"categories":null,"content":"","date":1363651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363651200,"objectID":"dcb88e46d2fcf3d1ce650c94a7d2be6c","permalink":"http://azarcone.github.io/publication/2013-iwcs/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2013-iwcs/","section":"publication","summary":"Logical metonymy interpretation (e.g. begin the book → writing) has received wide attention in linguistics. Experimental results have shown higher processing costs for metonymic conditions compared with non-metonymic ones (read the book). According to a widely held interpretation, it is the type clash between the event-selecting verb and the entity-denoting object (begin the book) that triggers coercion mechanisms and leads to additional processing effort. We propose an alternative explanation and argue that the extra processing effort is an effect of thematic fit. This is a more economical hypothesis that does not need to postulate a separate type clash mechanism: entity-denoting objects simply have a low fit as objects of event-selecting verbs. We test linguistic datasets from psycholinguistic experiments and find that a structured distributional model of thematic fit, which does not encode any explicit argument type information, is able to replicate all significant experimental findings. This result provides evidence for a graded account of coercion phenomena in which thematic fit accounts for both the trigger of the coercion and the retrieval of the covert event.","tags":["Complement Coercion","distributional semantics","expectation-based processing","Logical Metonymy","lexical meaning in context","type clash","typicality","world knowledge"],"title":"Fitting, not clashing! A distributional semantic model of logical metonymy","type":"publication"},{"authors":["Jason Utt","Alessandro Lenci","Sebastian Padó","Alessandra Zarcone"],"categories":null,"content":"","date":1363651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363651200,"objectID":"1f9708590f4f35062f9bbf396469dfd4","permalink":"http://azarcone.github.io/publication/2013-tfds/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2013-tfds/","section":"publication","summary":"Logical metonymy combines an event-selecting verb with an entity-denoting noun (e.g., The writer began the novel), triggering a covert event interpretation (e.g., reading, writing). Experimental investigations of logical metonymy must assume a binary distinction between metonymic (i.e. event-selecting) verbs and non-metonymic verbs to establish a control condition. However, this binary distinction (whether a verb is metonymic or not) is mostly made on intuitive grounds, which introduces a potential confounding factor. We describe a corpus-based approach which characterizes verbs in terms of their behavior at the syntax-semantics interface. The model assesses the extent to which transitive verbs prefer event-denoting objects over entity-denoting objects. We then test this “eventhood” measure on psycholinguistic datasets, showing that it can distinguish not only metonymic from non-metonymic verbs, but that it can also capture more fine-grained distinctions among different classes of metonymic verbs, putting such distinctions into a new graded perspective.","tags":["Complement Coercion","distributional semantics","expectation-based processing","Logical Metonymy","lexical meaning in context","type clash","typicality","world knowledge"],"title":"The curious case of metonymic verbs: A distributional characterization","type":"publication"},{"authors":[],"categories":null,"content":"","date":1356087600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356087600,"objectID":"97dfe66bfbc23881b7dc416945b976cd","permalink":"http://azarcone.github.io/talk/dimmi-con-che-parole-vai-e-ti-diro-chi-sei-modelli-distribuzionali-del-significato-ed-esperimenti-psicolinguistici/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/dimmi-con-che-parole-vai-e-ti-diro-chi-sei-modelli-distribuzionali-del-significato-ed-esperimenti-psicolinguistici/","section":"event","summary":"Presentazione sui modelli distribuzionali del significato e sulla loro validazione psicolinguistica.","tags":[],"title":"Dimmi con che parole vai e ti dirò chi sei: modelli distribuzionali del significato ed esperimenti psicolinguistici","type":"event"},{"authors":[],"categories":null,"content":"","date":1354183200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1354183200,"objectID":"fc8bb9848c25b18f7307a43176d8c465","permalink":"http://azarcone.github.io/talk/logical-metonymy-resolution-in-a-words-as-cues-framework-beyond-type-clash/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/logical-metonymy-resolution-in-a-words-as-cues-framework-beyond-type-clash/","section":"event","summary":"Talk on complement coercion through the lens of the word-as-cues framework","tags":[],"title":"Logical metonymy resolution in a words-as-cues framework: beyond type-clash","type":"event"},{"authors":["Alessandra Zarcone","Sebastian Padó","Alessandro Lenci"],"categories":null,"content":"","date":1346457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1346457600,"objectID":"8ec9de82432ad4b7857fbc63426b4a7d","permalink":"http://azarcone.github.io/publication/2012-cogsci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2012-cogsci/","section":"publication","summary":"It has been widely acknowledged that the interpretation of logical metonymies involves the interpretation of covert events (begin the book → reading / writing). Whether this implicit content is part of our lexicon or rather derives from general pragmatic inference, it is currently subject of debate. We present results from a probe recognition experiment, providing novel evidence in support of early metonymy processing, consistent with the hypothesis that covert events are retrieved from knowledge of typical events activated by lexical items.","tags":["complement coercion","expectation-based processing","generalized event knowledge","logical metonymy","lexical meaning in context","probe recognition","type clash","typicality","world knowledge"],"title":"Inferring Covert Events in Logical Metonymies: a Probe Recognition Experiment","type":"publication"},{"authors":[],"categories":null,"content":"","date":1339509600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1339509600,"objectID":"f9d3029b34dd77a94276c6e442c611a9","permalink":"http://azarcone.github.io/talk/event-knowledge-and-typicality-in-logical-metonymy-beyond-type-clash/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/event-knowledge-and-typicality-in-logical-metonymy-beyond-type-clash/","section":"event","summary":"Talk on the plausibility-driven interpretation of covert events.","tags":[],"title":"Event knowledge and typicality in logical metonymy: beyond type clash","type":"event"},{"authors":["Alessandra Zarcone","Jason Utt","Sebastian Padó"],"categories":null,"content":"","date":1339027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1339027200,"objectID":"85c9991cf204c79da483972868e8def8","permalink":"http://azarcone.github.io/publication/2012-cmcl/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2012-cmcl/","section":"publication","summary":"Logical metonymies (The student finished the beer) represent a challenge to compositionality since they involve semantic content not overtly realized in the sentence (covert events → drinking the beer). We present a contrastive study of two classes of computational models for logical metonymy in German, namely a probabilistic and a distributional, similarity-based model. These are built using the SDEWAC corpus and evaluated against a dataset from a self-paced reading and a probe recognition study for their sensitivity to thematic fit effects via their accuracy in predicting the correct covert event in a metonymical context. The similarity-based models allow for better coverage while maintaining the accuracy of the probabilistic models.","tags":["complement coercion","distributional semantics","logical metonymy","lexical meaning in context","probabilistic models","type clash","typicality","world knowledge"],"title":"Modeling covert event retrieval in logical metonymy: probabilistic and distributional accounts","type":"publication"},{"authors":["Stefan Rüd","Alessandra Zarcone"],"categories":null,"content":"","date":1337558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1337558400,"objectID":"a6d7d4c273c4d4282cdca6349c950a9d","permalink":"http://azarcone.github.io/publication/2012-lrec/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2012-lrec/","section":"publication","summary":"Logical metonymies like The author began the book involve the interpretation of events that are not realized in the sentence (Covert events: - writing the book). The Generative Lexicon (Pustejovsky 1995) provides a qualia-based account of covert event interpretation, claiming that the covert event is retrieved from the qualia structure of the object. Such a theory poses the question of to what extent covert events in logical metonymies can be accounted for by qualia structures. Building on previous work on English, we present a corpus study for German verbs (anfangen (mit), aufhören (mit), beenden, beginnen (mit), geniessen, based on data obtained from the deWaC corpus. We built a corpus of logical metonymies, which were manually annotated and compared with the qualia structures of their objects, then we contrasted annotation results from two expert annotators for metonymies (The author began the book) and long forms (The author began reading the book) across verbs. Our annotation was evaluated on a sample of sentences annotated by a group of naive annotators on a crowdsourcing platform. The logical metonymy database (2661 metonymies and 1886 long forms) with two expert annotations is freely available for scientific research purposes.","tags":["annotation","complement coercion","logical metonymy","lexical meaning in context","type clash"],"title":"Logical metonymies and qualia structures: an annotated database of logical metonymies for German","type":"publication"},{"authors":["Olga Batiukova","Pier Marco Bertinetto","Alessandro Lenci","Alessandra Zarcone"],"categories":null,"content":"","date":1326412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1326412800,"objectID":"eb4bc58ec9c2fd66ff06da035a94207b","permalink":"http://azarcone.github.io/publication/2012-russianverb/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2012-russianverb/","section":"publication","summary":"This paper reports four priming experiments, in which resultative, processual, and delimitative Russian verbs were tested.  The experiments were based on the semantic decision task: the participants had to decide whether the target denoted an event / situation with a clear outcome. To assess the impact of morphological cues on the decision latencies, verbs of different morphological complexity (prefixed and unprefixed perfectives) were used. The results obtained suggest that the aspectual feature of resultativity is consistently exploited in semantic priming (processual targets were primed in two experiments), and that the morphological cues facilitate the identification of resultative targets (prefixed perfectives exhibited faster decision latencies than unprefixed perfectives). As far as the delimitative forms are concerned, a category-induction experiment was designed to investigate the subjects’ tendency to group them with resultatives or with processuals, since the delimitatives represent an in-between category. The proportion of yes/no answers confirmed that the speakers place the delimitatives between these two domains, but much closer to the processuals than to the resultatives. These findings support the distinction of boundedness vs. telicity from both the theoretical and the behavioural perspective. The fact that the resultative interpretation of the delimitatives was not ruled out completely for most verbs suggests that, when certain conditions are met (when no cognate resultative form is readily available and when the delimitative is frequent enough), the delimitative can be conceptualized as the perfective counterpart of the basic imperfective, thus taking on the prototypical perfective role (resultativity).","tags":["aspectual features","lexical aspect","lexical meaning in context","semantic priming"],"title":"Semantic priming study of Russian aspect and resultativity","type":"publication"},{"authors":[],"categories":null,"content":"","date":1323166500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1323166500,"objectID":"67754a6a738337eb782beb2215b39eb0","permalink":"http://azarcone.github.io/talk/metodi-di-crowdsourcing-nello-studio-del-linguaggio/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/metodi-di-crowdsourcing-nello-studio-del-linguaggio/","section":"event","summary":"Presentazione in occasione di un evento per i dottorandi in linguistica dell'Università di Pisa.","tags":[],"title":"Metodi di crowdsourcing nello studio del linguaggio","type":"event"},{"authors":["Raymond Becker","Alan Cienki","Austin Bennett","Christina Cudina","Camille Debras","Zuzanna Fleischer","Michael Haaheim","Torsten Müller","Kashmiri Stec","Alessandra Zarcone"],"categories":null,"content":"","date":1315180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1315180800,"objectID":"140bb5450af47bd51a3c9cc21cbc910f","permalink":"http://azarcone.github.io/publication/2011-gespin/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2011-gespin/","section":"publication","summary":"Two studies were conducted to investigate the relationship between the production and comprehension of Aktionsarten (verbally expressed event structure, also called lexical aspect) in speech and gesture. Study 1, a qualitative/observational study, demonstrates discrete gesturing styles for each of the three major Aktionsarten categories. Study 2, a comprehension study, shows sensitivity to multimodal representations of event structure. Participants were both more accurate and faster at verifying the verb when gesture and speech conveyed compatible event structure representation than when they did not. Together, these results demonstrate a coherent conceptualization of event structure which supports theories of thinking-for-speaking in relation to gesture","tags":["aspectual features","lexical aspect","gesture"],"title":"Aktionsarten, speech and gesture","type":"publication"},{"authors":["Alessandra Zarcone","Sebastian Padó"],"categories":null,"content":"","date":1311120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1311120000,"objectID":"becb488740e957e1002d80da6ad1d922","permalink":"http://azarcone.github.io/publication/2011-cogsci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2011-cogsci/","section":"publication","summary":"The interpretation of logical metonymies like begin the book has traditionally been explained by assuming the existence of complex lexical entries containing information about event knowledge (qualia roles: reading the book/writing the book). Qualia structure provides concrete constraints on interpretation, which are however too rigid to be cognitively plausible. We suggest generalized event knowledge as an alternative source of interpretation. Results from a first self-paced reading experiment, where we capitalize on the verb-final word order in German subordinate phrases to create rich expectations for events, are presented to support this hypothesis. Consequences of this hypothesis for the interpretation logical metonymies are (a), it is primarily driven by pragmatic and world knowledge; (b), it may use the same (rather than distinct) mechanisms and resources as general incremental sentence comprehension does.","tags":["complement coercion","expectation-based processing","generalized event knowledge","logical metonymy","lexical meaning in context","self-paced reading","type clash","typicality","world knowledge"],"title":"Generalized Event Knowledge in Logical Metonymy Resolution","type":"publication"},{"authors":["Olga Batiukova","Pier Marco Bertinetto","Alessandro Lenci","Alessandra Zarcone"],"categories":null,"content":"","date":1303084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1303084800,"objectID":"30083c88f8498f775ce4cc18d9b88ea1","permalink":"http://azarcone.github.io/publication/2011-chronos/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2011-chronos/","section":"publication","summary":"This abstract reports and contrasts two psycholinguistic studies of durativity and resultativity in Italian and Russian. Unlike most experimental research, focused on ontological properties of aspectual classes of predicates (Todorova et al. 2000, Gennari \u0026 Poeppel 2003, Pylkkänen \u0026 McElree 2006, Bott 2008), we adopt feature-based approach to verbal semantics. Semantic priming as “improvement in speed or accuracy to respond to a stimulus, when it is preceded by a semantically related stimulus” (McNamara 2005) is used to show that a given feature is present in semantic representation of lexical meaning. We followed the general design in Bonnotte (2008), who tested French achievement and activity verbs. The author concluded that only positively-valued features benefit from priming: activities and achievements facilitated the processing of activities in the durativity task (where targets denoting durable situations had to be detected), whereas in the resultativity task (where situations with a clear outcome had to be detected) priming was displayed on achievements with achievement primes. Thus, durative verbs yield priming more easily than resultatives, presumably because durativity is a continuous feature and durativity a binary one. In our first study, Italian achievement and activity infinitives were used. The following pattern emerged: achievements were primed in both tasks and activities in the durativity task, always with activity primes. The only feature tested in Russian was resultativity; the data included simple imperfectives, and resultative and delimitative perfectives. Some interesting differences with respect to Italian were observed. For instance, facilitating priming was detected on activities only, confirming Bonnotte’s conclusions on their contextual adaptability. Furthermore, although the decision latencies are comparable, accuracy was higher in Russian: 0.93 vs. 0.86. The grammaticalized nature of Russian aspect and morphological cues (prefixes) certainly facilitated the task. Addition of tense morphology had similar effects in Italian: imperfect speeded up the processing of activities, and perfect the processing of achievements. Russian delimitatives (e.g. po-rabotat’ ‘work for a while’) yielded revealing findings, since they are bounded but atelic and thus represent an in-between category. The proportion of yes/no answers (0.43/0.57) in resultativity task suggests that the speakers place them between both domains, closer to activities than to resultatives. This supports the differentiation between boundedness and telicity from theoretical and behavioural perspective. Pairwise comparison of these results (and others still in progress) can help providing meaningful generalizations on the nature of durativity and resultativity in Romance and Slavic, and also verifying their empirical status, at the core of much theoretical work on aspect (Dölling et al. 2008).","tags":["aspectual features","lexical aspect","lexical meaning in context","semantic priming"],"title":"What semantic priming experiments tell us about aspectual features: cross-linguistic comparison","type":"publication"},{"authors":["Alessandra Zarcone","Alessandro Lenci"],"categories":null,"content":"","date":1288828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1288828800,"objectID":"01e03e09cc2812c7c3059e344c0f37ad","permalink":"http://azarcone.github.io/publication/2010-verb2/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2010-verb2/","section":"publication","summary":"Event types (ET) have received considerable attention in formal semantics, but their importance in experimental linguistics has developed only recently. The aim of this work is to compare the performance of human annotators and corpus-based models in ET classification of Italian verbs","tags":["annotation","aspectual features","crowdsourcing","lexical aspect","lexical meaning in context","Maximum Enthropy","Self-Organizing Maps"],"title":"Event Types in the Mind and in the Corpus","type":"publication"},{"authors":["Alessandra Zarcone","Sebastian Padó"],"categories":null,"content":"","date":1288828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1288828800,"objectID":"dd9f48b3f3e8b622e4a0fc9a63980d3e","permalink":"http://azarcone.github.io/publication/2010-verb1/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2010-verb1/","section":"publication","summary":"A range of event-subcategorizing verbs can combine with entity-denoting nouns, like begin the newspaper. The interpretation of such sentences typically involves the recovery of covert events (CE) which are not realized on the surface, as in .begin reading the newspaper.. We report on an ongoing study that scrutinizes two assumptions made by traditional accounts: (a) that the triggering of CEs can be ascribed to the object’s ontological type; and (b), that one or two CEs can be retrieved for each noun. Preliminary evidence against both assumptions is presented.","tags":["annotation","complement coercion","crowdsourcing","logical metonymy","lexical meaning in context","plausibility","type clash"],"title":"“I like work: I can sit and look at it for hours” - Type clash vs. plausibility in covert event recovery","type":"publication"},{"authors":["Alessandra Zarcone","Alessandro Lenci"],"categories":null,"content":"","date":1281484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1281484800,"objectID":"f8dab9b593ea0ed3c726407bb72ce85e","permalink":"http://azarcone.github.io/publication/2010-cogsci/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2010-cogsci/","section":"publication","summary":"Event types (ET) have been widely addressed in linguistic literature, but few studies have dealt with the questions of how they are represented, retrieved and processed in the mental lexicon. We report two experiments in which ET categories were found to give rise to semantic priming effects, both with word and picture stimuli. These effects are argued to provide empirical correlates for ET categories in the mental lexicon not only at the lexical level but also at a deeper conceptual level.","tags":["aspectual features","lexical aspect","lexical meaning in context","semantic priming"],"title":"Priming Effects on Event Types Classification: Effects of Word and Picture Stimuli","type":"publication"},{"authors":["Alessandra Zarcone"],"categories":null,"content":"","date":1237248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1237248000,"objectID":"9aaf432bd3fbc85554711dd354104e99","permalink":"http://azarcone.github.io/publication/2009-mathesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2009-mathesis/","section":"publication","summary":"Event types (ET) have been widely addressed in linguistics literature, but have received little attention in psycholinguitics, neurolinguistics and computational linguistics research. This thesis dissertation explores the nature of event types from a cognitive point of view: many descriptions and diagnostics on event types are available, but few studies have dealt with the problem of how event types are represented and processed in the mental lexicon. An important prerequisite for this sort of research is the building of a corpus of stimuli that meets our needs (web-based pre-tests were run to test the reliability of the stimuli, which should be balanced to control the variables known to affect processing costs) and an analysis of pre-existing literature in experimental psycholinguistics of event types. Our main concern was to explore new experimental settings in verb semantics psycholinguistics and to adapt them to this specific type of investigation: the choice of the method was narrowed down to the semantic priming paradigm, although the set of stimuli could also be suitable for other experimental settings, such as reading-time studies. The semantic priming paradigm was exploited to contrast processing effects on achievement verbs and activity verbs, which differ with respect to two superordinate features: durativity and resultativity. A series of priming experiments were run to explore differences and interactions between such features and the tense morphology and to evaluate the different contribution of the experimental setting in the observation and measurement of the effect: experiment 1 and experiment 2 followed a similar design and contrasted the effects of different neutral primes; experiment 3 focused on the interaction between event types and Italian tense morphology","tags":["aspectual features","lexical aspect","lexical meaning in context","semantic priming"],"title":"Empirical correlates of event types - a priming study","type":"publication"},{"authors":["Alessandro Lenci","Alessandra Zarcone"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"1d06c911b6be4ea6ccd22fea8e0d79ef","permalink":"http://azarcone.github.io/publication/2009-sli/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2009-sli/","section":"publication","summary":"Il fatto che uno stesso verbo possa avere valori azionali diversi a seconda del suo contesto linguistico solleva il problema di come modellare la complessa interazione dei fattori costituivi dell’Aktionsart. Per nessuna classe azionale sembra possibile selezionare un insieme di tratti la cui presenza in un contesto sia congiuntamente necessaria e sufficiente a garantire che l’evento venga interpretato come appartenente a quella particolare classe. Inoltre, gli stessi tipi azionali non si presentano come entità monolitiche, bensì come categorie che contengono rappresentanti verbali prototipici resistenti a variazioni contestuali, accanto invece a verbi che più facilmente danno luogo a fenomeni di polisemia a livello azionale. L’ipotesi di ricerca che esploriamo in questo lavoro è che l’interpretazione del valore azionale di un verbo in contesto possa essere modellata come il risultato di un complesso processo di integrazione di vincoli morfologici, sintattici e semantici di natura intrinsecamente probabilistica. Il peso relativo dei diversi vincoli viene stimato attraverso un algoritmo di apprendimento automatico basato sul principio della “massimizzazione dell’entropia”, che registra le correlazioni tra le classi azionali con diversi tratti del contesto linguistico dei verbi estratti da un corpus annotato.","tags":["aspectual features","lexical aspect","lexical meaning in context","Maximum Enthropy"],"title":"Un modello stocastico della classificazione azionale","type":"publication"},{"authors":["Alessandra Zarcone","Alessandro Lenci"],"categories":null,"content":"","date":1211932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1211932800,"objectID":"e1c00b40bbc5b598a45105e69f620e79","permalink":"http://azarcone.github.io/publication/2008-lrec/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2008-lrec/","section":"publication","summary":"Verb lexical semantic properties are only one of the factors that contribute to the determination of the event type expressed by a sentence, which is instead the result of a complex interplay between the verb meaning and its linguistic context. We report on two computational models for the automatic identification of event type in Italian. Both models use linguistically-motivated features extracted from Italian corpora. The main goal of our experiments is to evaluate the contribution of different types of linguistic indicators to identify the event type of a sentence, as well as to model various cases of context-driven event type shift. In the first model, event type identification has been modelled as a supervised classification task, performed with Maximum Entropy classifiers. In the second model, Self-Organizing Maps have been used to define and identify event types in an unsupervised way. The interaction of various contextual factors in determining the event type expressed by a sentence makes event type identification a highly challenging task. Computational models can help us to shed new light on the real structure of event type classes as well as to gain a better understanding of context-driven semantic shifts.","tags":["aspectual features","lexical aspect","lexical meaning in context","Maximum Enthropy","Self-Organizing Maps"],"title":"Computational Models for Event Type Classification in Context","type":"publication"},{"authors":["Alessandra Zarcone"],"categories":null,"content":"","date":1160611200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1160611200,"objectID":"f8f3d73e73d07a48cff9bb9f992dc9c0","permalink":"http://azarcone.github.io/publication/2006-bscthesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2006-bscthesis/","section":"publication","summary":"La presente relazione illustra i primi risultati di un'analisi computazionale dell'azionalità del verbo italiano, seguendo l'ipotesi (presentata per l'inglese americano da Siegel e McKeown (2000)) che specifici indicatori contestuali presenti in una frase possano fornire delle informazioni rilevanti sulla categoria azionale del verbo della frase stessa. Un modello computazionale della classificazione azionale può rivelarsi particolarmente interessante per il trattamento di fenomeni come le opposizioni infralessicali e la commutazione azionale. Il modello di classificazione azionale adottato è quello introdotto da Zeno Vendler, con particolare attenzione anche all'elaborazione teorica alle problematiche specifiche del verbo italiano presentate da Pier Marco Bertinetto. Il modello sperimentale adottato è l'apprendimento automatico supervisionato detto Maximum Entropy (principio della massimizzazione dell'entropia - Berger et al. 1996). L'uso dell'apprendimento automatico può essere un modo innovativo di modellizzare il problema della classificazione azionale, ma può anche fornire consistenza sperimentale alle correlazioni teoriche tra marche contestuali e categorie azionali: la rilevanza delle diverse marche contestuali è valutata in base ai risultati ottenuti dall'agente in grado di apprendere, che, dopo una prima fase di addestramento, deve decidere circa la categoria azionale di una determinata forma verbale. Un analizzatore di questo tipo sfrutta soltanto elementi presenti nel contesto, senza l'ausilio di informazioni semantiche interne al verbo stesso, nè conoscenze afferenti al dominio della pragmatica, che spesso nell'uomo hanno un ruolo rilevante.","tags":["aspectual features","human upper bound","lexical aspect","lexical meaning in context","Maximum Enthropy"],"title":"La classificazione azionale del verbo italiano: primi esperimenti computazionali","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"http://azarcone.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]